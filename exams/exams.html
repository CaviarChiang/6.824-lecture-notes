<h2>MapReduce</h2>

<p>Computation model, remember:</p>

<ul>
<li>input file is split <code>M</code> ways, </li>
<li>each split is sent to a <code>Map</code>,</li>
<li>each <code>Map()</code> returns a list of key-value pairs
<ul>
<li>map1 outputs {(k1, v1), (k2, v2)}</li>
<li>map2 outputs {(k1, v3), (k3, v4)}</li>
</ul></li>
<li>key value pairs from <code>Map</code> calls are merged</li>
<li>reduce is called on each key and its values
<ul>
<li>reduce1 input is {(k1, {v1,v3})}</li>
<li>reduce2 input is {(k2, {v2})}</li>
<li>reduce3 input is {(k3, {v4})}</li>
</ul></li>
<li>can you have a reduce job start before all maps are finished?
<ul>
<li>seems like it (see <a href="https://ercoppa.github.io/HadoopInternals/AnatomyMapReduceJob.html">here</a>)</li>
<li>actually seems like not (see <a href="https://stackoverflow.com/questions/11672676/when-do-reduce-tasks-start-in-hadoop">here</a></li>
<li>the reduce for key <code>k</code> can work on an iterator for 
the list of values associated with <code>k</code>
<ul>
<li>instead of receiving the full list</li>
</ul></li>
<li>as more map calls finish the iterator will have more 
values to return for that key</li>
</ul></li>
</ul>

<h2>RPCs</h2>

<ul>
<li><em>at least once:</em> send RPC req., wait for reply, retry if no reply
<ul>
<li>RPC calls are repeated <code>=&gt;</code> needs side-effect free RPCs to work correctly</li>
<li>ordering can be messed up: <code>send(req1); send(req2); ack(req2); Nack(req1);
resend(req1); ack(req1)</code>
<ul>
<li><code>req1</code> was sent before <code>req2</code> but was executed after <code>req2</code></li>
</ul></li>
</ul></li>
<li><em>at most once:</em> send RPC req. with XID, server executes RPC, remembers it by
its XID, never executes it again if it receives it again (just replies with 
remembered result)
<ul>
<li>no retries</li>
<li>discarding XIDs at the server side can be tricky</li>
</ul></li>
<li><em>exactly once:</em> at most once, + retries, + fault tolerance (to ensure no
corruption and hence <em>exactly once</em> semantics)</li>
</ul>

<h2>Primary-backup replication</h2>

<p>When is view allowed to change?</p>

<p><strong>TODO:</strong> Harp, understand how primary forwards ops to backups and/or witnesses.
what happens when some of them fail, etc.</p>

<p><strong>TODO:</strong> Flat data center storage. Blob ID + tract # are mapped to a tract entry. In FDS there are <code>O(n^2)</code> tract entries. 3 servers per entry. All possible combinations. Why?</p>

<ul>
<li><p>Why <code>O(n^2)</code>? We want replication => need 2 servers per TLT entry</p>

<ul>
<li>simple, but slow recovery: <code>n</code> entries, TLT entry <code>i</code>: server <code>i</code>, sever <code>i+1</code>
<ul>
<li>when a server <code>i</code> fails, only 2 others have its data
<ul>
<li><code>i-1</code> and <code>i+1</code></li>
</ul></li>
</ul></li>
<li>better: have <code>O(n^2)</code> entries so that every pair occurs in the TLT
<ul>
<li>when a disk <code>i</code> fails, it occurs in <code>n-1</code> pairs 
with other <code>n-1</code> servers
<ul>
<li>can use this to copy data from <code>n-1</code> disks at the same time</li>
</ul></li>
<li>the problem: if a 2nd disk fails at the same time, then we lose data
<ul>
<li>because there will be no way to get the data for the pair
formed by these two failed disks</li>
</ul></li>
</ul></li>
<li>even better: <code>O(n^2)</code> entries, all pairs of servers, and 
for every pair, if doing k-level replication (<code>k &gt; 2</code>), we add
k-2 randomly picked servers to each entry's pair</li>
</ul></li>
<li><p>But how are they mapped on disk? </p>

<ul>
<li>For now assume a dictionary/btree structure. </li>
</ul></li>
<li>When replacing a failed server, how is data transfer done? 
<ul>
<li>Is it just copied from the other servers in the TLT entry? </li>
<li>How is a replacement picked? (Randomly apparently) </li>
<li>Is the replacement server moved from its old TLT entry to the new one, or is it also left in the old TLT entry as well?
<ul>
<li>Figure 2 from paper suggests it is left in the old TLT entry</li>
</ul></li>
</ul></li>
</ul>

<p><strong>TODO:</strong> Paxos, try to understand why np/na are needed and what happened if one of them was not used.</p>

<p><strong>TODO:</strong> Raft, all of it.</p>

<p><strong>TODO:</strong> Go's memory model</p>

<p>The actual Go memory model is as follows:
A read r of a variable v is allowed to observe a write w to v if both of the following hold:</p>

<ol>
<li>r does not happen before w.</li>
<li>There is no other write w to v that happens after w but before r.</li>
</ol>

<p>To guarantee that a read r of a variable v observes a particular write w to v, ensure that w is the
only write r is allowed to observe. That is, r is guaranteed to observe w if both of the following
hold:</p>

<ol>
<li>w happens before r.</li>
<li>Any other write to the shared variable v either happens before w or after r.</li>
</ol>

<p>This pair of conditions is stronger than the first pair; it requires that there are no other writes
happening concurrently with w or r.</p>

<p>Within a single goroutine, there is no concurrency, so the two definitions are equivalent: a read r
observes the value written by the most recent write w to v. When multiple goroutines access a
shared variable v, they must use synchronization events to establish happens-before conditions
that ensure reads observe the desired writes.</p>

<p><strong>TODO:</strong> Write amplification vs. false sharing</p>

<p><strong>TODO:</strong> TreadMarks: lazy release consistency (different than ERC) + causal consistency</p>

<p><strong>TODO:</strong> Causal consistency: Does it ensure previous writes that did NOT contribute to current write
are also made visible?</p>

<ul>
<li>Q1 2009, Question 11 seems to suggest yes.</li>
</ul>

<p><strong>TODO:</strong> Vector timestamps and causal consistency</p>

<p><strong>TODO:</strong> Sequential consistency is going to be on the exam!</p>

<p><strong>TODO:</strong> The AnalogicFS paper, read it very carefully and understand it fully; it will definitely show up on the final.</p>
