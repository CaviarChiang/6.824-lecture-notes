<p><strong>TODO:</strong> Harp, understand how primary forwards ops to backups and/or witnesses. What happens when some of them fail, etc.</p>

<p><strong>TODO:</strong> Flat data center storage. Blob ID + tract # are mapped to a tract entry. In FDS there are <code>O(n^2)</code> tract entries. 3 servers per entry. All possible combinations. Why?</p>

<ul>
<li><p>Why <code>O(n^2)</code>? We want replication => need 2 servers per TLT entry</p>

<ul>
<li>simple, but slow recovery: <code>n</code> entries, TLT entry <code>i</code>: server <code>i</code>, sever <code>i+1</code>
<ul>
<li>when a server <code>i</code> fails, only 2 others have its data
<ul>
<li><code>i-1</code> and <code>i+1</code></li>
</ul></li>
</ul></li>
<li>better: have <code>O(n^2)</code> entries so that every pair occurs in the TLT
<ul>
<li>when a disk <code>i</code> fails, it occurs in <code>n-1</code> pairs 
with other <code>n-1</code> servers
<ul>
<li>can use this to copy data from <code>n-1</code> disks at the same time</li>
</ul></li>
<li>the problem: if a 2nd disk fails at the same time, then we lose data
<ul>
<li>because there will be no way to get the data for the pair
formed by these two failed disks</li>
</ul></li>
</ul></li>
<li>even better: <code>O(n^2)</code> entries, all pairs of servers, and 
for every pair, if doing k-level replication (<code>k &gt; 2</code>), we add
k-2 randomly picked servers to each entry's pair</li>
</ul></li>
<li><p>But how are they mapped on disk? </p>

<ul>
<li>For now assume a dictionary/btree structure. </li>
</ul></li>
<li>When replacing a failed server, how is data transfer done? 
<ul>
<li>Is it just copied from the other servers in the TLT entry? </li>
<li>How is a replacement picked? (Randomly apparently) </li>
<li>Is the replacement server moved from its old TLT entry to the new one, or is it also left in the old TLT entry as well?
<ul>
<li>Figure 2 from paper suggests it is left in the old TLT entry</li>
</ul></li>
</ul></li>
</ul>

<p><strong>TODO:</strong> Primary backup replication, remind yourself when view is allowed to change.</p>

<p><strong>TODO:</strong> Paxos, try to understand why np/na are needed and what happened if one of them was not used.</p>

<p><strong>TODO:</strong> Raft, all of it.</p>

<p><strong>TODO:</strong> Go's memory model</p>

<p>The actual Go memory model is as follows:
A read r of a variable v is allowed to observe a write w to v if both of the following hold:</p>

<ol>
<li>r does not happen before w.</li>
<li>There is no other write w to v that happens after w but before r.</li>
</ol>

<p>To guarantee that a read r of a variable v observes a particular write w to v, ensure that w is the
only write r is allowed to observe. That is, r is guaranteed to observe w if both of the following
hold:</p>

<ol>
<li>w happens before r.</li>
<li>Any other write to the shared variable v either happens before w or after r.</li>
</ol>

<p>This pair of conditions is stronger than the first pair; it requires that there are no other writes
happening concurrently with w or r.</p>

<p>Within a single goroutine, there is no concurrency, so the two definitions are equivalent: a read r
observes the value written by the most recent write w to v. When multiple goroutines access a
shared variable v, they must use synchronization events to establish happens-before conditions
that ensure reads observe the desired writes.</p>

<p><strong>TODO:</strong> Write amplification vs. false sharing</p>

<p><strong>TODO:</strong> TreadMarks: lazy release consistency (different than ERC) + causal consistency</p>

<p><strong>TODO:</strong> Causal consistency: Does it ensure previous writes that did NOT contribute to current write
are also made visible?</p>

<ul>
<li>Q1 2009, Question 11 seems to suggest yes.</li>
</ul>

<p><strong>TODO:</strong> Vector timestamps and causal consistency</p>

<p><strong>TODO:</strong> Map reduce computation model, remember:
 - input file is split M ways, 
 - each split is sent to a <code>Map</code>,
 - each <code>Map()</code> returns a list of key-value pairs
   - map1 outputs {(k1, v1), (k2, v2)}
   - map2 outputs {(k1, v3), (k3, v4)}
 - key value pairs from <code>Map</code> calls are merged
 - reduce is called on each key and its values
   - reduce1 input is {(k1, {v1,v3})}
   - reduce2 input is {(k2, {v2})}
   - reduce3 input is {(k3, {v4})}</p>

<p><strong>TODO:</strong> Sequential consistency is going to be on the exam!</p>

<p><strong>TODO:</strong> The AnalogicFS paper, read it very carefully and understand it fully; it will definitely show up on the final.</p>
